# Platonic Transformer ğŸ˜

Welcome to the Platonic Transformer project, where geometric group theory meets modern attention architectures ğŸŒŸ. This repository contains research code for Platonic-Group-equivariant Transformers that operate on scalar and vector features defined over the nodes of Platonic solids. We combine group convolutions and (linear) attention to deliver accurate, symmetry-aware models. ğŸ˜„

## Why You May Love This Project ğŸ™‚
- **Group-equivariant attention** that respects the symmetry group of the chosen Platonic solid (tetrahedron, octahedron, icosahedron ğŸ§Š).
- **Unified scalar/vector processing** with shared Platonic blocks for graph- and node-level predictions.
- **Supports multiple benchmarks** including CIFAR-10, ImageNet-1k, QM9 regression, ModelNet40, ShapeNet Cars, Protein folding, and Open Molecule Learning.
- **WandB integration** for centralized experiment tracking with teammates.

## Repository Tour ğŸ‘€
```
.
â”œâ”€â”€ datasets/                # Dataset wrappers and loaders for supported benchmarks
â”œâ”€â”€ main_*.py                # Entry points for training on specific datasets (CIFAR-10, QM9, etc.)
â”œâ”€â”€ models/
â”‚   â””â”€â”€ platoformer/         # Platonic Transformer building blocks
â”‚       â”œâ”€â”€ block.py         # Core PlatonicBlock attention + feedforward module
â”‚       â”œâ”€â”€ conv.py          # Group convolution utilities
â”‚       â”œâ”€â”€ groups.py        # Symmetry group definitions for Platonic solids
â”‚       â”œâ”€â”€ io.py            # Lifting, pooling, dense/sparse utilities
â”‚       â”œâ”€â”€ linear.py        # Equivariant linear projections
â”‚       â””â”€â”€ platoformer.py   # Full PlatonicTransformer module
â”œâ”€â”€ utils.py                # Shared training utilities (logging, metrics, augmentation)
â”œâ”€â”€ setup.sh                # Environment bootstrapper
â””â”€â”€ readme.md               # You are here ğŸ˜‡
```

## Getting Started ğŸš€

1. **Clone the repository** and install system dependencies if needed.
2. **Create the environment:**
   ```bash
   chmod +x setup.sh
   ./setup.sh
   ```
3. **Activate the environment:**
   ```bash
   source .venv/bin/activate
   ```
4. **Authenticate with Weights & Biases** :
   ```bash
   wandb login
   ```

## Training Workflows ğŸ‹ï¸â€â™€ï¸
Each `main_*.py` script exposes dataset-specific defaults. Pass `--help` for full CLI options, or keep things simple with the out-of-the-box settings:

Most scripts share common flags such as:
- `--solid_name {tetrahedron, octahedron, icosahedron}` to pick the symmetry group.
- `--hidden_dim`, `--layers`, `--num_heads` for model capacity.
- `--rope_sigma` / `--ape_sigma` to toggle rotational and absolute positional embeddings.
- `--conditioning_dim` when injecting diffusion or guidance signals.

Tip: start with smaller `--hidden-dim` (e.g., 64) and fewer layers to validate pipelines quickly ğŸ˜Š.

## Platonic Transformer Anatomy ğŸ§ 
The heart of the project sits in `models/platoformer/platoformer.py`:
- **Lifting:** `lift` maps scalar and vector node features to group-aligned channels.
- **Attention Blocks:** `PlatonicBlock` layers (stacked in `self.layers`) combine group-aware attention and equivariant MLPs with optional AdaLayerNorm-style conditioning.
- **Positional Encoding:** Choose between rotational positional encodings (RoPE) and absolute encodings (APE) tuned per solid.
- **Readout:** Separate scalar/vector readouts followed by pooling yield graph-level or node-level predictions, configurable via `scalar_task_level` and `vector_task_level`.


